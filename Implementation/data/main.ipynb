{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Twin of 5G network - Bachelor Thesis\n",
    "## Dataset Creation - David Truhlar, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.297333Z",
     "iopub.status.busy": "2025-03-24T12:00:42.296921Z",
     "iopub.status.idle": "2025-03-24T12:00:42.503227Z",
     "shell.execute_reply": "2025-03-24T12:00:42.502967Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "import pytz\n",
    "from pathlib import Path\n",
    "from metrics_config import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "PROMETHEUS_PORT = 9090                                                                            # Port for Prometheus metrics\n",
    "\n",
    "STEP = \"1s\"                                                                                   # Time step for the simulation\n",
    "TIMEDELTA_SECONDS = 10                                                                            # Time delta for the simulation\n",
    "LOCAL_TZ = pytz.timezone(\"Europe/Bratislava\")                                                     # TZ\n",
    "LOG_DIR = \"../log/\"                                                                               # Directory with logs\n",
    "LOG_PATTERN = re.compile(r\"(\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}):\\s+\\[(\\w+)\\]\\s+(\\w+):\\s*(.+)\")  # Regex pattern for log lines\n",
    "ENCODING = \"utf-8\"                                                                                # Encoding for log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.507419Z",
     "iopub.status.busy": "2025-03-24T12:00:42.507345Z",
     "iopub.status.idle": "2025-03-24T12:00:42.509272Z",
     "shell.execute_reply": "2025-03-24T12:00:42.509026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a connection to prometheus\n",
    "try:\n",
    "    prom = PrometheusConnect(url=f\"http://localhost:{PROMETHEUS_PORT}\", disable_ssl=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Prometheus: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time \n",
    "end_time = datetime.now()\n",
    "start_time = end_time - timedelta(seconds=TIMEDELTA_SECONDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# # Input range (UTC)\n",
    "# start_str = \"2025-04-11 10:22:00\"\n",
    "# end_str = \"2025-04-11 10:32:59\"\n",
    "\n",
    "# # Parse as UTC datetime\n",
    "# start_utc = datetime.fromisoformat(start_str)\n",
    "# end_utc = datetime.fromisoformat(end_str)\n",
    "\n",
    "# # Convert to local time with microseconds\n",
    "# start_local = start_utc.astimezone().replace(microsecond=0)\n",
    "# end_local = end_utc.astimezone().replace(microsecond=0)\n",
    "\n",
    "# print(\"Start time:\", start_utc)\n",
    "# print(\"End time:\", end_utc)\n",
    "\n",
    "# end_time = end_utc\n",
    "# start_time = start_utc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and process metrics from Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.526353Z",
     "iopub.status.busy": "2025-03-24T12:00:42.526276Z",
     "iopub.status.idle": "2025-03-24T12:00:42.664443Z",
     "shell.execute_reply": "2025-03-24T12:00:42.664179Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty list to hold metric DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Iterate through metric groups\n",
    "for group, metric_list in metrics.items():\n",
    "    for metric in metric_list:\n",
    "        try:\n",
    "            response = prom.custom_query_range(\n",
    "                metric, start_time=start_time, end_time=end_time, step=STEP\n",
    "            )\n",
    "\n",
    "            if not response:\n",
    "                print(f\"⚠️ No data: {group}/{metric}\")\n",
    "                continue\n",
    "\n",
    "            for entry in response:\n",
    "                base_metric_name = entry[\"metric\"].get(\"__name__\", metric)\n",
    "\n",
    "                if \"values\" in entry and isinstance(entry[\"values\"], list):\n",
    "                    extracted_values = [\n",
    "                        (\n",
    "                            datetime.utcfromtimestamp(int(ts))\n",
    "                            .replace(tzinfo=pytz.utc)\n",
    "                            .astimezone(LOCAL_TZ),\n",
    "                            float(val)\n",
    "                        )\n",
    "                        for ts, val in entry[\"values\"]\n",
    "                    ]\n",
    "\n",
    "                    metric_df = pd.DataFrame(extracted_values, columns=[\"timestamp\", \"value\"])\n",
    "                    metric_df[\"metric_name\"] = base_metric_name\n",
    "                    metric_df[\"group\"] = group\n",
    "\n",
    "                    df_list.append(metric_df)\n",
    "                else:\n",
    "                    print(f\"⚠️ No valid values found: {group}/{metric}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching {group}/{metric}: {e}\")\n",
    "\n",
    "# Combine all metrics into one DataFrame\n",
    "if df_list:\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    final_df['timestamp'] = final_df['timestamp'].astype(str).str.replace(r'\\+\\d{2}:\\d{2}', '', regex=True)\n",
    "    final_df[\"timestamp\"] = pd.to_datetime(final_df[\"timestamp\"])\n",
    "else:\n",
    "    print(\"❌ No data collected for any metric.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and extract logs from Open5gs functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.728463Z",
     "iopub.status.busy": "2025-03-24T12:00:42.728390Z",
     "iopub.status.idle": "2025-03-24T12:00:42.834558Z",
     "shell.execute_reply": "2025-03-24T12:00:42.834244Z"
    }
   },
   "outputs": [],
   "source": [
    "log_data = []\n",
    "\n",
    "# Define allowed applications\n",
    "allowed_applications = {\"amf\", \"upf\", \"smf\", \"udm\", \"gmm\"}\n",
    "\n",
    "for log_path in Path(LOG_DIR).glob(\"*.log\"):\n",
    "    try:\n",
    "        with open(log_path, \"r\", encoding=ENCODING, errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                match = LOG_PATTERN.match(line)\n",
    "                if match:\n",
    "                    timestamp_str, application, log_level, log_message = match.groups()\n",
    "\n",
    "                    # Filter by allowed applications\n",
    "                    if application.lower() not in allowed_applications:\n",
    "                        continue\n",
    "\n",
    "                    # Convert timestamp to datetime (add missing year)\n",
    "                    log_timestamp = datetime.strptime(timestamp_str, \"%m/%d %H:%M:%S.%f\")\n",
    "                    log_timestamp = log_timestamp.replace(year=start_time.year, microsecond=0)\n",
    "\n",
    "                    # Keep only logs after Prometheus start time\n",
    "                    if log_timestamp > start_time and log_timestamp < end_time:\n",
    "                        log_data.append({\n",
    "                            \"timestamp\": log_timestamp,\n",
    "                            \"application\": application,\n",
    "                            \"log_level\": log_level,\n",
    "                            \"log_message\": log_message\n",
    "                        })\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to process {log_path.name}: {e}\")\n",
    "\n",
    "log_data = pd.DataFrame(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate and transform metrics data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.880500Z",
     "iopub.status.busy": "2025-03-24T12:00:42.880428Z",
     "iopub.status.idle": "2025-03-24T12:00:42.889189Z",
     "shell.execute_reply": "2025-03-24T12:00:42.888966Z"
    }
   },
   "outputs": [],
   "source": [
    "# 🔹 Aggregate metrics (choose appropriate aggregation: mean, sum, max, etc.)\n",
    "data_agg = final_df.groupby([\"timestamp\", \"metric_name\"])[\"value\"].mean().reset_index()\n",
    "data_pivot = data_agg.pivot(index=\"timestamp\", columns=\"metric_name\", values=\"value\")\n",
    "\n",
    "# Flatten column names\n",
    "data_pivot.columns = [f\"{col}_value\" for col in data_pivot.columns]\n",
    "\n",
    "# Reset index to bring timestamp back\n",
    "data_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = \"test_merged_data3.csv\"\n",
    "\n",
    "# # Determine whether to write header\n",
    "# write_header = not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0\n",
    "\n",
    "# # Try filtering only new records if file exists and is not empty\n",
    "# if not write_header:\n",
    "#     try:\n",
    "#         last_timestamp = pd.read_csv(csv_file, usecols=[\"timestamp\"])[\"timestamp\"].max()\n",
    "#         data_pivot = data_pivot[data_pivot[\"timestamp\"] > last_timestamp]\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Issue reading existing CSV: {e}. Proceeding without filtering.\")\n",
    "\n",
    "# # Append new data\n",
    "# if not data_pivot.empty:\n",
    "#     data_pivot.to_csv(csv_file, mode=\"a\", index=False, header=write_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.893116Z",
     "iopub.status.busy": "2025-03-24T12:00:42.893046Z",
     "iopub.status.idle": "2025-03-24T12:00:42.894955Z",
     "shell.execute_reply": "2025-03-24T12:00:42.894735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to classify log messages\n",
    "patterns = {\n",
    "    \"remove\": re.compile(r\"\\b(Removed|Deregister|De-register|Implicit De-registered)\\b\", re.IGNORECASE),\n",
    "    \"refused\": re.compile(r\"\\b(refused|connection refused)\\b\", re.IGNORECASE),\n",
    "    \"number_of_sessions_or_ues\": re.compile(r\"\\b(Number of (gNBs|AMF-UEs|AMF-Sessions|gNB-UEs))\\b\", re.IGNORECASE),\n",
    "    \"registration\": re.compile(r\"\\b(Registration request|InitialUEMessage|Added|Unknown UE by SUCI|SUCI)\\b\", re.IGNORECASE),\n",
    "    \"error\": re.compile(r\"\\b(ERROR)\\b\", re.IGNORECASE),\n",
    "    \"warning\": re.compile(r\"\\b(WARNING)\\b\", re.IGNORECASE),\n",
    "}\n",
    "\n",
    "def classify_log_message(message):\n",
    "    if not isinstance(message, str):\n",
    "        return \"nothing\"\n",
    "    for label, pattern in patterns.items():\n",
    "        if pattern.search(message):\n",
    "            return label\n",
    "    return \"nothing\"\n",
    "\n",
    "# Ensure log_message column exists before applying classification\n",
    "if \"log_message\" in log_data.columns:\n",
    "\tlog_data[\"log_type\"] = log_data[\"log_message\"].apply(classify_log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.901685Z",
     "iopub.status.busy": "2025-03-24T12:00:42.901613Z",
     "iopub.status.idle": "2025-03-24T12:00:42.903499Z",
     "shell.execute_reply": "2025-03-24T12:00:42.903288Z"
    }
   },
   "outputs": [],
   "source": [
    "logs_short = log_data[[\"timestamp\", \"application\", \"log_type\"]] \\\n",
    "\tif not log_data.empty \\\n",
    "\telse pd.DataFrame(columns=[\"timestamp\", \"application\", \"log_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.907731Z",
     "iopub.status.busy": "2025-03-24T12:00:42.907659Z",
     "iopub.status.idle": "2025-03-24T12:00:42.909949Z",
     "shell.execute_reply": "2025-03-24T12:00:42.909717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert logs_short timestamp to UTC datetime (assumes naive timestamps are UTC)\n",
    "logs_short[\"timestamp\"] = pd.to_datetime(logs_short[\"timestamp\"], errors='coerce').dt.tz_localize(\"UTC\")\n",
    "\n",
    "# Filter logs after the UTC start_time (also localized to UTC for comparison)\n",
    "logs_short = logs_short[logs_short[\"timestamp\"] >= start_time.replace(tzinfo=pytz.utc)]\n",
    "\n",
    "# Drop timezone info to match Prometheus format\n",
    "logs_short[\"timestamp\"] = logs_short[\"timestamp\"].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge logs with metrics\n",
    "merged_data = pd.merge(data_pivot, logs_short, on=\"timestamp\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7b/kthfybh105s_tz_wbjp8ntjr0000gn/T/ipykernel_85743/1007763271.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_data[\"application\"] = merged_data[\"application\"].fillna(0).astype(str)  # Ensure 'application' is a string\n",
      "/var/folders/7b/kthfybh105s_tz_wbjp8ntjr0000gn/T/ipykernel_85743/1007763271.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_data[\"log_type\"] = merged_data[\"log_type\"].fillna(0).astype(str)        # Ensure 'log_type' is a string\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values in 'application' and 'log_type' columns with 0\n",
    "merged_data[\"application\"] = merged_data[\"application\"].fillna(0)\n",
    "merged_data[\"log_type\"] = merged_data[\"log_type\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the current_uc value\n",
    "current_uc = None\n",
    "\n",
    "with open(\"../current_uc.txt\", \"r\") as f:\n",
    "    current_uc = f.read().strip()\n",
    "\n",
    "# Add current_uc as a new column to the merged_data DataFrame\n",
    "if not merged_data.empty:\n",
    "    merged_data[\"current_uc\"] = current_uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.929414Z",
     "iopub.status.busy": "2025-03-24T12:00:42.929343Z",
     "iopub.status.idle": "2025-03-24T12:00:42.933314Z",
     "shell.execute_reply": "2025-03-24T12:00:42.933111Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_file = \"running_data.csv\"\n",
    "\n",
    "# Determine whether to write header\n",
    "write_header = not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0\n",
    "\n",
    "# Try filtering only new records if file exists and is not empty\n",
    "if not write_header:\n",
    "    try:\n",
    "        last_timestamp = pd.read_csv(csv_file, usecols=[\"timestamp\"])[\"timestamp\"].max()\n",
    "        merged_data = merged_data[merged_data[\"timestamp\"] > last_timestamp]\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Issue reading existing CSV: {e}. Proceeding without filtering.\")\n",
    "\n",
    "# Append new data\n",
    "if not merged_data.empty:\n",
    "    merged_data.to_csv(csv_file, mode=\"a\", index=False, header=write_header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
