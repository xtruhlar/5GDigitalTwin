{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Twin of 5G network - Bachelor Thesis\n",
    "## Dataset Creation - David Truhlar, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.297333Z",
     "iopub.status.busy": "2025-03-24T12:00:42.296921Z",
     "iopub.status.idle": "2025-03-24T12:00:42.503227Z",
     "shell.execute_reply": "2025-03-24T12:00:42.502967Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "import pytz\n",
    "from pathlib import Path\n",
    "from metrics_config import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "PROMETHEUS_PORT = 9090                                                                            # Port for Prometheus metrics\n",
    "\n",
    "STEP = \"1s\"                                                                                   # Time step for the simulation\n",
    "TIMEDELTA_SECONDS = 10                                                                            # Time delta for the simulation\n",
    "LOCAL_TZ = pytz.timezone(\"Europe/Bratislava\")                                                     # TZ\n",
    "LOG_DIR = \"../log/\"                                                                               # Directory with logs\n",
    "LOG_PATTERN = re.compile(r\"(\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}):\\s+\\[(\\w+)\\]\\s+(\\w+):\\s*(.+)\")  # Regex pattern for log lines\n",
    "ENCODING = \"utf-8\"                                                                                # Encoding for log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.507419Z",
     "iopub.status.busy": "2025-03-24T12:00:42.507345Z",
     "iopub.status.idle": "2025-03-24T12:00:42.509272Z",
     "shell.execute_reply": "2025-03-24T12:00:42.509026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Prometheus\n"
     ]
    }
   ],
   "source": [
    "# Create a connection to prometheus\n",
    "try:\n",
    "    prom = PrometheusConnect(url=f\"http://localhost:{PROMETHEUS_PORT}\", disable_ssl=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Prometheus: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time \n",
    "end_time = datetime.now()\n",
    "start_time = end_time - timedelta(seconds=TIMEDELTA_SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and process metrics from Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.526353Z",
     "iopub.status.busy": "2025-03-24T12:00:42.526276Z",
     "iopub.status.idle": "2025-03-24T12:00:42.664443Z",
     "shell.execute_reply": "2025-03-24T12:00:42.664179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No data: fivegs_amf/fivegs_amffunction_rm_registeredsubnbr\n",
      "‚ö†Ô∏è No data: fivegs_smf/fivegs_smffunction_sm_pdusessioncreationreq\n",
      "‚ö†Ô∏è No data: fivegs_smf/fivegs_smffunction_sm_pdusessioncreationsucc\n",
      "‚ö†Ô∏è No data: fivegs_smf/fivegs_smffunction_sm_qos_flow_nbr\n",
      "‚ö†Ô∏è No data: fivegs_smf/fivegs_smffunction_sm_sessionnbr\n",
      "‚ö†Ô∏è No data: fivegs_upf/fivegs_upffunction_upf_qosflows\n",
      "‚ö†Ô∏è No data: fivegs_pcf/fivegs_pcffunction_pa_policyamassoreq\n",
      "‚ö†Ô∏è No data: fivegs_pcf/fivegs_pcffunction_pa_policyamassosucc\n",
      "‚ö†Ô∏è No data: fivegs_pcf/fivegs_pcffunction_pa_policysmassoreq\n",
      "‚ö†Ô∏è No data: fivegs_pcf/fivegs_pcffunction_pa_policysmassosucc\n",
      "‚ö†Ô∏è No data: fivegs_pcf/fivegs_pcffunction_pa_sessionnbr\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to hold metric DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Iterate through metric groups\n",
    "for group, metric_list in metrics.items():\n",
    "    for metric in metric_list:\n",
    "        try:\n",
    "            response = prom.custom_query_range(\n",
    "                metric, start_time=start_time, end_time=end_time, step=STEP\n",
    "            )\n",
    "\n",
    "            if not response:\n",
    "                print(f\"‚ö†Ô∏è No data: {group}/{metric}\")\n",
    "                continue\n",
    "\n",
    "            for entry in response:\n",
    "                base_metric_name = entry[\"metric\"].get(\"__name__\", metric)\n",
    "\n",
    "                if \"values\" in entry and isinstance(entry[\"values\"], list):\n",
    "                    extracted_values = [\n",
    "                        (\n",
    "                            datetime.utcfromtimestamp(int(ts))\n",
    "                            .replace(tzinfo=pytz.utc)\n",
    "                            .astimezone(LOCAL_TZ),\n",
    "                            float(val)\n",
    "                        )\n",
    "                        for ts, val in entry[\"values\"]\n",
    "                    ]\n",
    "\n",
    "                    metric_df = pd.DataFrame(extracted_values, columns=[\"timestamp\", \"value\"])\n",
    "                    metric_df[\"metric_name\"] = base_metric_name\n",
    "                    metric_df[\"group\"] = group\n",
    "\n",
    "                    df_list.append(metric_df)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è No valid values found: {group}/{metric}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching {group}/{metric}: {e}\")\n",
    "\n",
    "# Combine all metrics into one DataFrame\n",
    "if df_list:\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    final_df['timestamp'] = final_df['timestamp'].astype(str).str.replace(r'\\+\\d{2}:\\d{2}', '', regex=True)\n",
    "    final_df[\"timestamp\"] = pd.to_datetime(final_df[\"timestamp\"])\n",
    "else:\n",
    "    print(\"‚ùå No data collected for any metric.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and extract logs from Open5gs functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.728463Z",
     "iopub.status.busy": "2025-03-24T12:00:42.728390Z",
     "iopub.status.idle": "2025-03-24T12:00:42.834558Z",
     "shell.execute_reply": "2025-03-24T12:00:42.834244Z"
    }
   },
   "outputs": [],
   "source": [
    "log_data = []\n",
    "\n",
    "for log_path in Path(LOG_DIR).glob(\"*.log\"):\n",
    "    try:\n",
    "        with open(log_path, \"r\", encoding=ENCODING, errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                match = LOG_PATTERN.match(line)\n",
    "                if match:\n",
    "                    timestamp_str, application, log_level, log_message = match.groups()\n",
    "\n",
    "                    # Convert timestamp to datetime (add missing year)\n",
    "                    log_timestamp = datetime.strptime(timestamp_str, \"%m/%d %H:%M:%S.%f\")\n",
    "                    log_timestamp = log_timestamp.replace(year=start_time.year, microsecond=0)\n",
    "\n",
    "                    # Keep only logs after Prometheus start time\n",
    "                    if log_timestamp > start_time:\n",
    "                        log_data.append({\n",
    "                            \"timestamp\": log_timestamp,\n",
    "                            \"application\": application,\n",
    "                            \"log_level\": log_level,\n",
    "                            \"log_message\": log_message\n",
    "                        })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to process {log_path.name}: {e}\")\n",
    "\n",
    "log_data = pd.DataFrame(log_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate and transform metrics data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.880500Z",
     "iopub.status.busy": "2025-03-24T12:00:42.880428Z",
     "iopub.status.idle": "2025-03-24T12:00:42.889189Z",
     "shell.execute_reply": "2025-03-24T12:00:42.888966Z"
    }
   },
   "outputs": [],
   "source": [
    "# üîπ Aggregate metrics (choose appropriate aggregation: mean, sum, max, etc.)\n",
    "data_agg = final_df.groupby([\"timestamp\", \"metric_name\"])[\"value\"].mean().reset_index()\n",
    "data_pivot = data_agg.pivot(index=\"timestamp\", columns=\"metric_name\", values=\"value\")\n",
    "\n",
    "# Flatten column names\n",
    "data_pivot.columns = [f\"{col}_value\" for col in data_pivot.columns]\n",
    "\n",
    "# Reset index to bring timestamp back\n",
    "data_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.893116Z",
     "iopub.status.busy": "2025-03-24T12:00:42.893046Z",
     "iopub.status.idle": "2025-03-24T12:00:42.894955Z",
     "shell.execute_reply": "2025-03-24T12:00:42.894735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'log_message' column not found in log_data DataFrame\n"
     ]
    }
   ],
   "source": [
    "# Define function to classify log messages\n",
    "patterns = {\n",
    "    \"connect\": re.compile(r\"connect\", re.IGNORECASE),\n",
    "    \"request\": re.compile(r\"request\", re.IGNORECASE),\n",
    "    \"reject\": re.compile(r\"reject\", re.IGNORECASE),\n",
    "}\n",
    "\n",
    "def classify_log_message(message):\n",
    "    if not isinstance(message, str):\n",
    "        return \"nothing\"\n",
    "    for label, pattern in patterns.items():\n",
    "        if pattern.search(message):\n",
    "            return label\n",
    "    return \"nothing\"\n",
    "\n",
    "# Ensure log_message column exists before applying classification\n",
    "if \"log_message\" in log_data.columns:\n",
    "\tlog_data[\"log_type\"] = log_data[\"log_message\"].apply(classify_log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.901685Z",
     "iopub.status.busy": "2025-03-24T12:00:42.901613Z",
     "iopub.status.idle": "2025-03-24T12:00:42.903499Z",
     "shell.execute_reply": "2025-03-24T12:00:42.903288Z"
    }
   },
   "outputs": [],
   "source": [
    "logs_short = log_data[[\"timestamp\", \"application\", \"log_type\"]] \\\n",
    "\tif not log_data.empty \\\n",
    "\telse pd.DataFrame(columns=[\"timestamp\", \"application\", \"log_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.907731Z",
     "iopub.status.busy": "2025-03-24T12:00:42.907659Z",
     "iopub.status.idle": "2025-03-24T12:00:42.909949Z",
     "shell.execute_reply": "2025-03-24T12:00:42.909717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert logs_short timestamp to UTC datetime (assumes naive timestamps are UTC)\n",
    "logs_short[\"timestamp\"] = pd.to_datetime(logs_short[\"timestamp\"], errors='coerce').dt.tz_localize(\"UTC\")\n",
    "\n",
    "# Filter logs after the UTC start_time (also localized to UTC for comparison)\n",
    "logs_short = logs_short[logs_short[\"timestamp\"] >= start_time.replace(tzinfo=pytz.utc)]\n",
    "\n",
    "# Drop timezone info to match Prometheus format\n",
    "logs_short[\"timestamp\"] = logs_short[\"timestamp\"].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge logs with metrics\n",
    "merged_data = pd.merge(data_pivot, logs_short, on=\"timestamp\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.929414Z",
     "iopub.status.busy": "2025-03-24T12:00:42.929343Z",
     "iopub.status.idle": "2025-03-24T12:00:42.933314Z",
     "shell.execute_reply": "2025-03-24T12:00:42.933111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ New data appended to CSV!\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"merged_data.csv\"\n",
    "\n",
    "# Determine whether to write header\n",
    "write_header = not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0\n",
    "\n",
    "# Try filtering only new records if file exists and is not empty\n",
    "if not write_header:\n",
    "    try:\n",
    "        last_timestamp = pd.read_csv(csv_file, usecols=[\"timestamp\"])[\"timestamp\"].max()\n",
    "        merged_data = merged_data[merged_data[\"timestamp\"] > last_timestamp]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Issue reading existing CSV: {e}. Proceeding without filtering.\")\n",
    "\n",
    "# Append new data\n",
    "if not merged_data.empty:\n",
    "    merged_data.to_csv(csv_file, mode=\"a\", index=False, header=write_header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
