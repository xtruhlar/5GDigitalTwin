{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tvorba datasetu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obsah<a class='anchor' id='top'></a>\n",
    "* [Globálne premenné](#globalne-premenne)\n",
    "* [Pripojenie na Prometheus](#pripojenie-na-prometheus)\n",
    "* [Časové vymedzenie](#casove-vymedzenie)\n",
    "* [Načítanie a spracovanie metrík z Prometheus](#nacitanie-a-spracovanie-metrik-z-prometheus)\n",
    "* [Spracovanie a extrakcia logov z funkcií Open5gs](#spracovanie-a-extrakcia-logov-z-funkcii-open5gs)\n",
    "* [Agregácia a transformácia dát](#agregacia-a-transformacia-dat)\n",
    "* [Triedenie logov](#triedenie-logov)\n",
    "* [Spojenie metrík a logov](#spojenie-metrik-a-logov)\n",
    "* [Mapovanie kategorických dát](#mapovanie-kategorickych-dat)\n",
    "* [Vloženie informácie o akutálnom UC](#vlozenie-informacie-o-akutalnom-uc)\n",
    "* [Uloženie datasetu](#ulozenie-datasetu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.297333Z",
     "iopub.status.busy": "2025-03-24T12:00:42.296921Z",
     "iopub.status.idle": "2025-03-24T12:00:42.503227Z",
     "shell.execute_reply": "2025-03-24T12:00:42.502967Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from prometheus_api_client import PrometheusConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"fivegs_smf\": [\n",
    "        \"fivegs_smffunction_sm_n4sessionreportsucc\",\n",
    "        \"fivegs_smffunction_sm_pdusessioncreationreq\",\n",
    "        \"fivegs_smffunction_sm_qos_flow_nbr\",\n",
    "    ],\n",
    "    \"fivegs_pcf\": [\n",
    "        \"fivegs_pcffunction_pa_policysmassosucc\",\n",
    "        \"fivegs_pcffunction_pa_sessionnbr\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "PROMETHEUS_PORT = 9090                                                                            # Port for Prometheus metrics\n",
    "STEP = \"1s\"                                                                                       # Time step for the simulation\n",
    "TIMEDELTA_SECONDS = 10                                                                            # Time delta for the simulation\n",
    "LOCAL_TZ = pytz.timezone(\"Europe/Bratislava\")                                                     # Timezone for the simulation\n",
    "LOG_DIR = \"/open5gs/install/var/log/open5gs\"                                                      # Directory with logs\n",
    "LOG_PATTERN = re.compile(r\"(\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}):\\s+\\[(\\w+)\\]\\s+(\\w+):\\s*(.+)\")  # Regex pattern for log lines\n",
    "ENCODING = \"utf-8\"                                                                                # Encoding for log files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Príklad logu:</b> 04/02 11:05:03.836: [amf] INFO: [Added] Number of gNB-UEs is now 3 (../src/amf/context.c:2678)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.507419Z",
     "iopub.status.busy": "2025-03-24T12:00:42.507345Z",
     "iopub.status.idle": "2025-03-24T12:00:42.509272Z",
     "shell.execute_reply": "2025-03-24T12:00:42.509026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Prometheus\n",
    "try:\n",
    "    prom = PrometheusConnect(url=f\"http://metrics:{PROMETHEUS_PORT}\", disable_ssl=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Prometheus: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Prometheus:</b> Pre správne fungovanie je potrebné sa uistiť, že Docker container 'metrics' je spustený a beží na porte 9090.<br>\n",
    "60f374bfb155   docker_metrics           \"/bin/bash -c /mnt/m…\"   2 days ago     Up 30 minutes   0.0.0.0:9090->9090/tcp   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time range for the simulation\n",
    "end_time = datetime.now()\n",
    "start_time = end_time - timedelta(seconds=TIMEDELTA_SECONDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input range (UTC)\n",
    "# start_str = \"2025-04-14 12:52:30\"\n",
    "# end_str = \"2025-04-14 12:56:30\"\n",
    "\n",
    "# # Parse as UTC datetime\n",
    "# start_utc = datetime.fromisoformat(start_str)\n",
    "# end_utc = datetime.fromisoformat(end_str)\n",
    "\n",
    "# # Convert to local time with microseconds\n",
    "# start_local = start_utc.astimezone().replace(microsecond=0)\n",
    "# end_local = end_utc.astimezone().replace(microsecond=0)\n",
    "\n",
    "# print(\"Start time:\", start_utc)\n",
    "# print(\"End time:\", end_utc)\n",
    "\n",
    "# end_time = end_utc\n",
    "# start_time = start_utc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Zakomentovaný kód:</b> Bunka vyššie slúžila na špecifikovanie časového rozsahu pre zber dát. Keď bol vytváraný reálny dataset, čas bol zaznamenaný, aby sme mohli neskôr načítať dáta zo servera Prometheus.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Načítanie a spracovanie metrík z Promethus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.526353Z",
     "iopub.status.busy": "2025-03-24T12:00:42.526276Z",
     "iopub.status.idle": "2025-03-24T12:00:42.664443Z",
     "shell.execute_reply": "2025-03-24T12:00:42.664179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No data: fivegs_smf/fivegs_smffunction_sm_pdusessioncreationreq\n",
      "⚠️ No data: fivegs_smf/fivegs_smffunction_sm_qos_flow_nbr\n",
      "⚠️ No data: fivegs_pcf/fivegs_pcffunction_pa_policysmassosucc\n",
      "⚠️ No data: fivegs_pcf/fivegs_pcffunction_pa_sessionnbr\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to hold metric DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Iterate through metric groups\n",
    "for group, metric_list in metrics.items():\n",
    "    for metric in metric_list:\n",
    "        try:\n",
    "            response = prom.custom_query_range(\n",
    "                metric, start_time=start_time, end_time=end_time, step=STEP\n",
    "            )\n",
    "\n",
    "            if not response:\n",
    "                print(f\"⚠️ No data: {group}/{metric}\")\n",
    "                continue\n",
    "\n",
    "            for entry in response:\n",
    "                base_metric_name = entry[\"metric\"].get(\"__name__\", metric)\n",
    "\n",
    "                if \"values\" in entry and isinstance(entry[\"values\"], list):\n",
    "                    extracted_values = [\n",
    "                        (\n",
    "                            datetime.utcfromtimestamp(int(ts))\n",
    "                            .replace(tzinfo=pytz.utc)\n",
    "                            .astimezone(LOCAL_TZ),\n",
    "                            float(val)\n",
    "                        )\n",
    "                        for ts, val in entry[\"values\"]\n",
    "                    ]\n",
    "\n",
    "                    metric_df = pd.DataFrame(extracted_values, columns=[\"timestamp\", \"value\"])\n",
    "                    metric_df[\"metric_name\"] = base_metric_name\n",
    "                    metric_df[\"group\"] = group\n",
    "\n",
    "                    df_list.append(metric_df)\n",
    "                else:\n",
    "                    print(f\"⚠️ No valid values found: {group}/{metric}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching {group}/{metric}: {e}\")\n",
    "\n",
    "# Combine all metrics into one DataFrame\n",
    "if df_list:\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    final_df['timestamp'] = final_df['timestamp'].astype(str).str.replace(r'\\+\\d{2}:\\d{2}', '', regex=True)\n",
    "    final_df[\"timestamp\"] = pd.to_datetime(final_df[\"timestamp\"])\n",
    "else:\n",
    "    print(\"❌ No data collected for any metric.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>final_df:</b> je dataframe, ktorý obsahuje všetky potrebné metriky z Prometheus.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spracovanie a extrakcia logov z funkcií Open5gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.728463Z",
     "iopub.status.busy": "2025-03-24T12:00:42.728390Z",
     "iopub.status.idle": "2025-03-24T12:00:42.834558Z",
     "shell.execute_reply": "2025-03-24T12:00:42.834244Z"
    }
   },
   "outputs": [],
   "source": [
    "log_data = []\n",
    "\n",
    "# Aplikácie, ktoré chceme sledovať\n",
    "allowed_applications = {\"amf\", \"upf\", \"smf\", \"udm\", \"gmm\"}\n",
    "\n",
    "for log_path in Path(LOG_DIR).glob(\"*.log\"):\n",
    "    try:\n",
    "        with open(log_path, \"r\", encoding=ENCODING, errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                match = LOG_PATTERN.match(line)\n",
    "                if match:\n",
    "                    timestamp_str, application, log_level, log_message = match.groups()\n",
    "\n",
    "                    if application.lower() not in allowed_applications:\n",
    "                        continue\n",
    "\n",
    "                    # Konvertujeme timestamp na datetime\n",
    "                    log_timestamp = datetime.strptime(timestamp_str, \"%m/%d %H:%M:%S.%f\")\n",
    "                    log_timestamp = log_timestamp.replace(year=start_time.year, microsecond=0)\n",
    "\n",
    "                    # Nechávame len logy v časovom intervale\n",
    "                    if log_timestamp > start_time and log_timestamp < end_time:\n",
    "                        log_data.append({\n",
    "                            \"timestamp\": log_timestamp,\n",
    "                            \"application\": application,\n",
    "                            \"log_level\": log_level,\n",
    "                            \"log_message\": log_message\n",
    "                        })\n",
    "                    else:\n",
    "                        continue\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to process {log_path.name}: {e}\")\n",
    "\n",
    "log_data = pd.DataFrame(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>log_data:</b> je dataframe, ktorý obsahuje všetky potrebné logy z Open5gs.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agregácia a transformácia údajov o metrikách"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.880500Z",
     "iopub.status.busy": "2025-03-24T12:00:42.880428Z",
     "iopub.status.idle": "2025-03-24T12:00:42.889189Z",
     "shell.execute_reply": "2025-03-24T12:00:42.888966Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data aggregation\n",
    "data_agg = final_df.groupby([\"timestamp\", \"metric_name\"])[\"value\"].mean().reset_index()\n",
    "data_pivot = data_agg.pivot(index=\"timestamp\", columns=\"metric_name\", values=\"value\")\n",
    "\n",
    "# Adding \"_value\" to column names\n",
    "data_pivot.columns = [f\"{col}_value\" for col in data_pivot.columns]\n",
    "\n",
    "# Adding a time column\n",
    "data_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.893116Z",
     "iopub.status.busy": "2025-03-24T12:00:42.893046Z",
     "iopub.status.idle": "2025-03-24T12:00:42.894955Z",
     "shell.execute_reply": "2025-03-24T12:00:42.894735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logs classification\n",
    "patterns = {\n",
    "    \"remove\": re.compile(r\"\\b(Removed|Deregister|De-register|Implicit De-registered)\\b\", re.IGNORECASE),\n",
    "    \"refused\": re.compile(r\"\\b(refused|connection refused)\\b\", re.IGNORECASE),\n",
    "    \"number_of_sessions_or_ues\": re.compile(r\"\\b(Number of (gNBs|AMF-UEs|AMF-Sessions|gNB-UEs))\\b\", re.IGNORECASE),\n",
    "    \"registration\": re.compile(r\"\\b(Registration request|InitialUEMessage|Added|Unknown UE by SUCI|SUCI)\\b\", re.IGNORECASE),\n",
    "    \"error\": re.compile(r\"\\b(ERROR)\\b\", re.IGNORECASE),\n",
    "    \"warning\": re.compile(r\"\\b(WARNING)\\b\", re.IGNORECASE),\n",
    "}\n",
    "\n",
    "def classify_log_message(message):\n",
    "    if not isinstance(message, str):\n",
    "        return \"nothing\"\n",
    "    for label, pattern in patterns.items():\n",
    "        if pattern.search(message):\n",
    "            return label\n",
    "    return \"nothing\"\n",
    "\n",
    "# Before adding the \"log_type\" column, check if the \"log_message\" column exists\n",
    "if \"log_message\" in log_data.columns:\n",
    "\tlog_data[\"log_type\"] = log_data[\"log_message\"].apply(classify_log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.901685Z",
     "iopub.status.busy": "2025-03-24T12:00:42.901613Z",
     "iopub.status.idle": "2025-03-24T12:00:42.903499Z",
     "shell.execute_reply": "2025-03-24T12:00:42.903288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shorten the DataFrame to only include relevant columns\n",
    "logs_short = log_data[[\"timestamp\", \"application\", \"log_type\"]] \\\n",
    "\tif not log_data.empty \\\n",
    "\telse pd.DataFrame(columns=[\"timestamp\", \"application\", \"log_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.907731Z",
     "iopub.status.busy": "2025-03-24T12:00:42.907659Z",
     "iopub.status.idle": "2025-03-24T12:00:42.909949Z",
     "shell.execute_reply": "2025-03-24T12:00:42.909717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logs are in UTC, so we need to convert them to the local timezone\n",
    "logs_short[\"timestamp\"] = pd.to_datetime(logs_short[\"timestamp\"], errors='coerce').dt.tz_localize(\"UTC\")\n",
    "\n",
    "# Logs filtering\n",
    "logs_short = logs_short[logs_short[\"timestamp\"] >= start_time.replace(tzinfo=pytz.utc)]\n",
    "\n",
    "# Remove timezone from timestamp to match the format in Prometheus data\n",
    "logs_short[\"timestamp\"] = logs_short[\"timestamp\"].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "logs_short = logs_short.drop_duplicates(subset=[\"timestamp\", \"application\", \"log_type\"])\n",
    "logs_short = logs_short.sort_values(by=[\"timestamp\", \"application\", \"log_type\"])\n",
    "logs_short = logs_short.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>logs_short:</b> tento dataframe budeme spájať s dataframe-om final_df.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics and logs merging\n",
    "merged_data = pd.merge(data_pivot, logs_short, on=\"timestamp\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Missing metrics in final DataFrame:\n",
      "  - fivegs_pcffunction_pa_sessionnbr_value\n",
      "  - fivegs_pcffunction_pa_policysmassosucc_value\n",
      "  - fivegs_smffunction_sm_pdusessioncreationreq_value\n",
      "  - fivegs_smffunction_sm_qos_flow_nbr_value\n"
     ]
    }
   ],
   "source": [
    "featured_metrics = {\"features\": [\"fivegs_smffunction_sm_n4sessionreportsucc_value\", \"fivegs_pcffunction_pa_sessionnbr_value\", \"fivegs_pcffunction_pa_policysmassosucc_value\", \"fivegs_smffunction_sm_pdusessioncreationreq_value\", \"fivegs_smffunction_sm_qos_flow_nbr_value\", \"log_type\", \"application\"]}\n",
    "featured_metrics = featured_metrics[\"features\"]\n",
    "\n",
    "missing_metrics = []\n",
    "for metric in featured_metrics:\n",
    "    if metric not in merged_data.columns:\n",
    "        missing_metrics.append(metric)\n",
    "if missing_metrics:\n",
    "    print(\"❌ Missing metrics in final DataFrame:\")\n",
    "    for metric in missing_metrics:\n",
    "        print(f\"  - {metric}\")\n",
    "\n",
    "# Add columns for missing metrics\n",
    "for metric in missing_metrics:\n",
    "    merged_data[metric] = 0.0\n",
    "\n",
    "\n",
    "# Order the columns to match the selected features\n",
    "ordered_columns = [\"timestamp\"] + featured_metrics\n",
    "for col in merged_data.columns:\n",
    "    if col not in ordered_columns:\n",
    "        ordered_columns.append(col)\n",
    "merged_data = merged_data[ordered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on predefined mappings we convert the application and log_type columns to numerical values\n",
    "APP_MAP = {\"0\": 0, \"amf\": 1, \"gmm\": 2, \"udm\": 3, \"smf\": 4, \"upf\": 5}\n",
    "LOG_MAP = {\"0\": 0, \"registration\": 1, \"number_of_sessions_or_ues\": 2, \"nothing\": 3, \"remove\": 4, \"error\": 5}\n",
    "\n",
    "merged_data['application'] = merged_data['application'].map(APP_MAP).fillna(0)\n",
    "merged_data['log_type'] = merged_data['log_type'].map(LOG_MAP).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_uc = None\n",
    "\n",
    "# Load the current UC from a file\n",
    "with open(\"./data/current_uc.txt\", \"r\") as f:\n",
    "    current_uc = f.read().strip()\n",
    "\n",
    "# If the current UC is valid, add it to the DataFrame\n",
    "if not merged_data.empty:\n",
    "    merged_data[\"current_uc\"] = current_uc\n",
    "\n",
    "UC_MAP = {\"uc1\": 0, \"uc2\": 1, \"uc3\": 2, \"uc4\": 3, \"uc5\": 4, \"uc6\": 5}\n",
    "\n",
    "# Convert the current_uc column to numerical values\n",
    "merged_data['current_uc'] = merged_data['current_uc'].map(UC_MAP).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b> Mapovanie:</b> je potrebné zabezpečiť aby bol súbor 'current_uc.txt' vytvorený a v správnom adresári."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T12:00:42.929414Z",
     "iopub.status.busy": "2025-03-24T12:00:42.929343Z",
     "iopub.status.idle": "2025-03-24T12:00:42.933314Z",
     "shell.execute_reply": "2025-03-24T12:00:42.933111Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_file = \"./data/running_data.csv\"\n",
    "\n",
    "# If the file doesn't exist or is empty, we write the header\n",
    "write_header = not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0\n",
    "\n",
    "# Filter only new records if file exists and is not empty\n",
    "if not write_header:\n",
    "    try:\n",
    "        last_timestamp = pd.read_csv(csv_file, usecols=[\"timestamp\"])[\"timestamp\"].max()\n",
    "        merged_data = merged_data[merged_data[\"timestamp\"] > last_timestamp]\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Issue reading existing CSV: {e}. Proceeding without filtering.\")\n",
    "\n",
    "# Add new data to the CSV file\n",
    "if not merged_data.empty:\n",
    "    merged_data.to_csv(csv_file, mode=\"a\", index=False, header=write_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Uloženie datasetu:</b> je potrebné zabezpečiť aby bol súbor 'running_data.csv' vytvorený a v správnom adresári.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
