{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c6ae24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5fd42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../real_data.csv\")\n",
    "scaler = joblib.load(\"scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "31e3e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Attention Layer (compatible with LSTM output)\n",
    "class AttentionLayer(Layer):\n",
    "    \n",
    "    \"\"\"Custom Attention Layer for LSTM models.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        \"\"\"Build the layer and define trainable weights for the attention mechanism.\"\"\"\n",
    "\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the attention scores and apply them to the input.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, time_steps, features).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Context vector of shape (batch_size, features).\n",
    "        \"\"\"\n",
    "\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)   # Compute attention scores\n",
    "        a = K.softmax(e, axis=1)                # Normalize attention scores\n",
    "        output = x * a                          # Apply attention scores\n",
    "        return K.sum(output, axis=1)            # Sum along the time axis\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute the output shape of the layer.\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): Shape of the input tensor.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Shape of the output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "759292e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_base = load_model('trained_models/lstm_base_model.h5')\n",
    "model_robust = load_model('trained_models/lstm_robust_model.h5')\n",
    "model_batchnorm = load_model('trained_models/lstm_batchnorm_model.h5')\n",
    "model_attention = load_model('trained_models/lstm_attention_model.h5', custom_objects={'AttentionLayer': AttentionLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "71655130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile the models\n",
    "model_base.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_robust.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_batchnorm.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_attention.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ac7d1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "UC_MAP = {\"uc1\": 0, \"uc2\": 1, \"uc3\": 2, \"uc4\": 3, \"uc5\": 4, \"uc6\": 5}\n",
    "APP_MAP = {'0': 0, 'amf': 1, 'gmm': 2, 'udm': 3, 'smf': 4, 'upf': 5}\n",
    "LOG_MAP = {'0': 0, 'registration': 1, 'number_of_sessions_or_ues': 2,\n",
    "           'nothing': 3, 'remove': 4, 'error': 5 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5953eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['current_uc'] = data['current_uc'].map(UC_MAP)\n",
    "data[\"application\"] = data[\"application\"].map(APP_MAP)\n",
    "data[\"log_type\"] = data[\"log_type\"].map(LOG_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "597176a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('selected_features.json', 'r') as f:\n",
    "    selected_features = json.load(f)['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2f0421df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[selected_features]\n",
    "y = data['current_uc']\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4075325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "real_data_class_weight_dict = dict(zip(np.unique(y), real_data_class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b7a57d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real data class weights: {np.int64(0): np.float64(0.3595654478007419), np.int64(1): np.float64(1.2806719516798792), np.int64(2): np.float64(0.8759359669506842), np.int64(3): np.float64(1.0105749180816206), np.int64(4): np.float64(13.960905349794238), np.int64(5): np.float64(4.251253132832081)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Real data class weights:\", real_data_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a08b8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, seq_len):\n",
    "\n",
    "    \"\"\"Create sequences of data with given length for LSTM input.\"\"\"\n",
    "\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        X_seq.append(X[i:i+seq_len])\n",
    "        y_seq.append(y[i+seq_len])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_real_seq, y_real_seq = create_sequences(X_scaled, y, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "409936be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_base_model = model_base.predict(X_real_seq)\n",
    "y_pred_base_model = y_pred_base_model.argmax(axis=1)\n",
    "\n",
    "y_pred_robust_model = model_robust.predict(X_real_seq)\n",
    "y_pred_robust_model = y_pred_robust_model.argmax(axis=1)\n",
    "\n",
    "y_pred_batchnorm_model = model_batchnorm.predict(X_real_seq)\n",
    "y_pred_batchnorm_model = y_pred_batchnorm_model.argmax(axis=1)\n",
    "\n",
    "y_pred_attention_model = model_attention.predict(X_real_seq)\n",
    "y_pred_attention_model = y_pred_attention_model.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "381adf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         uc1       0.00      0.00      0.00      3115\n",
      "         uc2       0.14      1.00      0.24       883\n",
      "         uc3       0.00      0.00      0.00      1291\n",
      "         uc4       0.00      0.00      0.00      1089\n",
      "         uc5       0.00      0.00      0.00        81\n",
      "         uc6       1.00      0.01      0.01       266\n",
      "\n",
      "    accuracy                           0.13      6725\n",
      "   macro avg       0.19      0.17      0.04      6725\n",
      "weighted avg       0.06      0.13      0.03      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Base model classification report:\")\n",
    "print(classification_report(y_real_seq, y_pred_base_model, target_names=list(UC_MAP.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "baea7a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust model classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         uc1       0.00      0.00      0.00      3115\n",
      "         uc2       0.00      0.00      0.00       883\n",
      "         uc3       0.00      0.00      0.00      1291\n",
      "         uc4       0.17      1.00      0.29      1089\n",
      "         uc5       0.00      0.00      0.00        81\n",
      "         uc6       0.00      0.00      0.00       266\n",
      "\n",
      "    accuracy                           0.16      6725\n",
      "   macro avg       0.03      0.17      0.05      6725\n",
      "weighted avg       0.03      0.16      0.05      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Robust model classification report:\")\n",
    "print(classification_report(y_real_seq, y_pred_robust_model, target_names=list(UC_MAP.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "562a4830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batchnorm model classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         uc1       0.00      0.00      0.00      3115\n",
      "         uc2       0.00      0.00      0.00       883\n",
      "         uc3       0.00      0.00      0.00      1291\n",
      "         uc4       0.00      0.00      0.00      1089\n",
      "         uc5       0.01      1.00      0.02        81\n",
      "         uc6       1.00      0.77      0.87       266\n",
      "\n",
      "    accuracy                           0.04      6725\n",
      "   macro avg       0.17      0.30      0.15      6725\n",
      "weighted avg       0.04      0.04      0.03      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Batchnorm model classification report:\")\n",
    "print(classification_report(y_real_seq, y_pred_batchnorm_model, target_names=list(UC_MAP.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4686dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention model classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         uc1       0.00      0.00      0.00      3115\n",
      "         uc2       0.00      0.00      0.00       883\n",
      "         uc3       0.20      1.00      0.33      1291\n",
      "         uc4       0.00      0.00      0.00      1089\n",
      "         uc5       0.00      0.00      0.00        81\n",
      "         uc6       1.00      0.96      0.98       266\n",
      "\n",
      "    accuracy                           0.23      6725\n",
      "   macro avg       0.20      0.33      0.22      6725\n",
      "weighted avg       0.08      0.23      0.10      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention model classification report:\")\n",
    "print(classification_report(y_real_seq, y_pred_attention_model, target_names=list(UC_MAP.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4ad6d",
   "metadata": {},
   "source": [
    "## Try finetuning with real network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9b463630",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = pd.read_csv(\"../real_data.csv\")\n",
    "\n",
    "real_data['application'] = real_data['application'].map(APP_MAP)\n",
    "real_data['log_type'] = real_data['log_type'].map(LOG_MAP)\n",
    "real_data['current_uc'] = real_data['current_uc'].map(UC_MAP)\n",
    "real_data['timestamp'] = pd.to_datetime(real_data['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "37058c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real = real_data[selected_features].values\n",
    "y_real = real_data['current_uc'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "008c59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the selected features match those used during scaler fitting\n",
    "X_real = real_data[scaler.feature_names_in_]\n",
    "X_real_scaled = scaler.transform(X_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3cff91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real_seq, y_real_seq = create_sequences(X_real_scaled, y_real, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2164ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft, X_val, y_ft, y_val = train_test_split(\n",
    "    X_real_seq, y_real_seq, test_size=0.8, stratify=y_real_seq, random_state=42\n",
    ")\n",
    "\n",
    "y_ft_cat = to_categorical(y_ft, len(np.unique(y_ft)))\n",
    "y_val_cat = to_categorical(y_val, len(np.unique(y_ft)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "012c6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attention.compile(\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "124ba9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.2267 - loss: 2.6557 - val_accuracy: 0.2314 - val_loss: 1.7527\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.1474 - loss: 1.7428 - val_accuracy: 0.1704 - val_loss: 1.8534\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.1992 - loss: 1.4371 - val_accuracy: 0.1704 - val_loss: 1.6976\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.1857 - loss: 1.4899 - val_accuracy: 0.1704 - val_loss: 1.6703\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.1893 - loss: 1.4402 - val_accuracy: 0.1704 - val_loss: 1.6128\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.1865 - loss: 1.4268 - val_accuracy: 0.1704 - val_loss: 1.5741\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.2292 - loss: 1.4416 - val_accuracy: 0.5024 - val_loss: 1.5469\n",
      "Epoch 8/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.2578 - loss: 1.3721 - val_accuracy: 0.1704 - val_loss: 1.5412\n",
      "Epoch 9/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.2137 - loss: 1.4039 - val_accuracy: 0.5024 - val_loss: 1.5211\n",
      "Epoch 10/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.3852 - loss: 1.4564 - val_accuracy: 0.5024 - val_loss: 1.5121\n"
     ]
    }
   ],
   "source": [
    "history = model_attention.fit(\n",
    "    X_ft,\n",
    "    y_ft_cat,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    class_weight=real_data_class_weight_dict,\n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(patience=50, restore_best_weights=True)],\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5adfbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Attention model classification report on real data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         uc1       0.48      1.00      0.65      3115\n",
      "         uc2       0.00      0.00      0.00       883\n",
      "         uc3       0.00      0.00      0.00      1291\n",
      "         uc4       0.00      0.00      0.00      1089\n",
      "         uc5       0.00      0.00      0.00        81\n",
      "         uc6       1.00      0.99      1.00       266\n",
      "\n",
      "    accuracy                           0.50      6725\n",
      "   macro avg       0.25      0.33      0.27      6725\n",
      "weighted avg       0.26      0.50      0.34      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_real_pred_attention = model_attention.predict(X_real_seq)\n",
    "y_real_pred_attention = y_real_pred_attention.argmax(axis=1)\n",
    "print(\"Attention model classification report on real data:\")\n",
    "print(classification_report(y_real_seq, y_real_pred_attention, target_names=list(UC_MAP.keys())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
