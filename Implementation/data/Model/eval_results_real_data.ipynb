{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6ae24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidtruhlar/Documents/FIIT/BP/5GDigitalTwin/Implementation/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5fd42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/real_network_data_after_labeling.csv\")\n",
    "scaler = joblib.load(\"./scaler/scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e3e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Attention Layer (compatible with LSTM output)\n",
    "class AttentionLayer(Layer):\n",
    "    \n",
    "    \"\"\"Custom Attention Layer for LSTM models.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        \"\"\"Build the layer and define trainable weights for the attention mechanism.\"\"\"\n",
    "\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the attention scores and apply them to the input.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, time_steps, features).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Context vector of shape (batch_size, features).\n",
    "        \"\"\"\n",
    "\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)   # Compute attention scores\n",
    "        a = K.softmax(e, axis=1)                # Normalize attention scores\n",
    "        output = x * a                          # Apply attention scores\n",
    "        return K.sum(output, axis=1)            # Sum along the time axis\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute the output shape of the layer.\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): Shape of the input tensor.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Shape of the output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759292e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_base = load_model('trained_models/lstm_base_model.h5')\n",
    "model_robust = load_model('trained_models/lstm_robust_model.h5')\n",
    "model_batchnorm = load_model('trained_models/lstm_batchnorm_model.h5')\n",
    "model_attention = load_model('trained_models/lstm_attention_model.h5', custom_objects={'AttentionLayer': AttentionLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71655130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile the models\n",
    "model_base.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_robust.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_batchnorm.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_attention.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac7d1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./json/uc_map.json\", \"r\") as f:\n",
    "    UC_MAP = json.load(f)\n",
    "with open(\"./json/app_map.json\", \"r\") as f:\n",
    "    APP_MAP = json.load(f)\n",
    "with open(\"./json/log_map.json\", \"r\") as f:\n",
    "    LOG_MAP = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5953eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['current_uc'] = data['current_uc'].map(UC_MAP)\n",
    "data[\"application\"] = data[\"application\"].map(APP_MAP)\n",
    "data[\"log_type\"] = data[\"log_type\"].map(LOG_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "597176a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./json/selected_features.json', 'r') as f:\n",
    "    selected_features = json.load(f)['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f0421df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[selected_features]\n",
    "y = data['current_uc']\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4075325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "real_data_class_weight_dict = dict(zip(np.unique(y), real_data_class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7a57d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real data class weights: {np.int64(0): np.float64(0.3595654478007419), np.int64(1): np.float64(1.2806719516798792), np.int64(2): np.float64(0.8759359669506842), np.int64(3): np.float64(1.0105749180816206), np.int64(4): np.float64(13.960905349794238), np.int64(5): np.float64(4.251253132832081)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Real data class weights:\", real_data_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a08b8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, seq_len):\n",
    "\n",
    "    \"\"\"Create sequences of data with given length for LSTM input.\"\"\"\n",
    "\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        X_seq.append(X[i:i+seq_len])\n",
    "        y_seq.append(y[i+seq_len])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_real_seq, y_real_seq = create_sequences(X_scaled, y, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "409936be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_base_model = model_base.predict(X_real_seq)\n",
    "y_pred_base_model = y_pred_base_model.argmax(axis=1)\n",
    "\n",
    "y_pred_robust_model = model_robust.predict(X_real_seq)\n",
    "y_pred_robust_model = y_pred_robust_model.argmax(axis=1)\n",
    "\n",
    "y_pred_batchnorm_model = model_batchnorm.predict(X_real_seq)\n",
    "y_pred_batchnorm_model = y_pred_batchnorm_model.argmax(axis=1)\n",
    "\n",
    "y_pred_attention_model = model_attention.predict(X_real_seq)\n",
    "y_pred_attention_model = y_pred_attention_model.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "381adf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         uc1       0.00      0.00      0.00      3115\n",
      "         uc2       0.14      1.00      0.24       883\n",
      "         uc3       0.00      0.00      0.00      1291\n",
      "         uc4       0.00      0.00      0.00      1089\n",
      "         uc5       0.00      0.00      0.00        81\n",
      "         uc6       1.00      0.13      0.23       266\n",
      "\n",
      "    accuracy                           0.14      6725\n",
      "   macro avg       0.19      0.19      0.08      6725\n",
      "weighted avg       0.06      0.14      0.04      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Base model classification report:\")\n",
    "print(classification_report(y_real_seq, y_pred_base_model, target_names=list(UC_MAP.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baea7a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust model classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         uc1       0.46      0.94      0.62      3115\n",
      "         uc2       0.00      0.00      0.00       883\n",
      "         uc3       0.00      0.00      0.00      1291\n",
      "         uc4       0.00      0.00      0.00      1089\n",
      "         uc5       0.00      0.00      0.00        81\n",
      "         uc6       1.00      0.07      0.13       266\n",
      "\n",
      "    accuracy                           0.44      6725\n",
      "   macro avg       0.24      0.17      0.12      6725\n",
      "weighted avg       0.25      0.44      0.29      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Robust model classification report:\")\n",
    "print(classification_report(y_real_seq, y_pred_robust_model, target_names=list(UC_MAP.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "562a4830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batchnorm model classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         uc1       0.31      0.46      0.37      3115\n",
      "         uc2       0.00      0.00      0.00       883\n",
      "         uc3       0.08      0.01      0.01      1291\n",
      "         uc4       0.00      0.00      0.00      1089\n",
      "         uc5       0.00      0.00      0.00        81\n",
      "         uc6       0.00      0.00      0.00       266\n",
      "\n",
      "    accuracy                           0.21      6725\n",
      "   macro avg       0.06      0.08      0.06      6725\n",
      "weighted avg       0.16      0.21      0.17      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Batchnorm model classification report:\")\n",
    "print(classification_report(y_real_seq, y_pred_batchnorm_model, target_names=list(UC_MAP.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4686dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention model classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         uc1       0.00      0.00      0.00      3115\n",
      "         uc2       0.00      0.00      0.00       883\n",
      "         uc3       0.00      0.00      0.00      1291\n",
      "         uc4       0.16      1.00      0.28      1089\n",
      "         uc5       0.00      0.00      0.00        81\n",
      "         uc6       0.00      0.00      0.00       266\n",
      "\n",
      "    accuracy                           0.16      6725\n",
      "   macro avg       0.03      0.17      0.05      6725\n",
      "weighted avg       0.03      0.16      0.05      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention model classification report:\")\n",
    "print(classification_report(y_real_seq, y_pred_attention_model, target_names=list(UC_MAP.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4ad6d",
   "metadata": {},
   "source": [
    "## Try finetuning with real network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b463630",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = pd.read_csv(\"../datasets/real_network_data_after_labeling.csv\")\n",
    "\n",
    "real_data['application'] = real_data['application'].map(APP_MAP)\n",
    "real_data['log_type'] = real_data['log_type'].map(LOG_MAP)\n",
    "real_data['current_uc'] = real_data['current_uc'].map(UC_MAP)\n",
    "real_data['timestamp'] = pd.to_datetime(real_data['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37058c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real = real_data[selected_features].values\n",
    "y_real = real_data['current_uc'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "008c59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the selected features match those used during scaler fitting\n",
    "X_real = real_data[scaler.feature_names_in_]\n",
    "X_real_scaled = scaler.transform(X_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cff91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real_seq, y_real_seq = create_sequences(X_real_scaled, y_real, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2164ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft, X_val, y_ft, y_val = train_test_split(\n",
    "    X_real_seq, y_real_seq, test_size=0.8, stratify=y_real_seq, random_state=42\n",
    ")\n",
    "\n",
    "y_ft_cat = to_categorical(y_ft, len(np.unique(y_ft)))\n",
    "y_val_cat = to_categorical(y_val, len(np.unique(y_ft)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "012c6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attention.compile(\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "124ba9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.1324 - loss: 8.6635 - val_accuracy: 0.2310 - val_loss: 1.6259\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.3001 - loss: 1.3430 - val_accuracy: 0.1701 - val_loss: 1.5717\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.2478 - loss: 1.4687 - val_accuracy: 0.0511 - val_loss: 1.6413\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.2046 - loss: 1.4399 - val_accuracy: 0.5022 - val_loss: 1.5501\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.3029 - loss: 1.4353 - val_accuracy: 0.1703 - val_loss: 1.5451\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.2332 - loss: 1.4694 - val_accuracy: 0.5024 - val_loss: 1.5545\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.2205 - loss: 1.4240 - val_accuracy: 0.5022 - val_loss: 1.5340\n",
      "Epoch 8/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.1841 - loss: 1.4106 - val_accuracy: 0.1704 - val_loss: 1.5565\n",
      "Epoch 9/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.1939 - loss: 1.4049 - val_accuracy: 0.2312 - val_loss: 1.5595\n",
      "Epoch 10/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.1871 - loss: 1.3899 - val_accuracy: 0.1704 - val_loss: 1.5680\n"
     ]
    }
   ],
   "source": [
    "history = model_attention.fit(\n",
    "    X_ft,\n",
    "    y_ft_cat,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    class_weight=real_data_class_weight_dict,\n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5adfbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Attention model classification report on real data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         uc1       0.48      1.00      0.65      3115\n",
      "         uc2       0.00      0.00      0.00       883\n",
      "         uc3       0.00      0.00      0.00      1291\n",
      "         uc4       0.00      0.00      0.00      1089\n",
      "         uc5       0.00      0.00      0.00        81\n",
      "         uc6       1.00      0.98      0.99       266\n",
      "\n",
      "    accuracy                           0.50      6725\n",
      "   macro avg       0.25      0.33      0.27      6725\n",
      "weighted avg       0.26      0.50      0.34      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_real_pred_attention = model_attention.predict(X_real_seq)\n",
    "y_real_pred_attention = y_real_pred_attention.argmax(axis=1)\n",
    "print(\"Attention model classification report on real data:\")\n",
    "print(classification_report(y_real_seq, y_real_pred_attention, target_names=list(UC_MAP.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74982506",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Výsledky modelu s Attention:</b> Model pozornosti bol vyhodnotený na reálnych dátach. Dosiahol presnosť 0.50 na testovacej množine, makro priemerná presnosť je 0.27 a vážená priemerná presnosť je 0.34. Výsledky modelu naznačujú, že môže efektívne zachytávať základné vzory v dátach.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a551b98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 161ms/step - accuracy: 0.3402 - loss: 5.2518 - val_accuracy: 0.0459 - val_loss: 1.6753\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 149ms/step - accuracy: 0.1301 - loss: 1.4948 - val_accuracy: 0.0480 - val_loss: 1.6057\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 155ms/step - accuracy: 0.1614 - loss: 1.4837 - val_accuracy: 0.1671 - val_loss: 1.6307\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 0.1403 - loss: 1.4967 - val_accuracy: 0.1697 - val_loss: 1.6475\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 0.1572 - loss: 1.5037 - val_accuracy: 0.1704 - val_loss: 1.5815\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 155ms/step - accuracy: 0.1865 - loss: 1.4414 - val_accuracy: 0.2312 - val_loss: 1.5670\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 159ms/step - accuracy: 0.2248 - loss: 1.3363 - val_accuracy: 0.1704 - val_loss: 1.5354\n",
      "Epoch 8/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 159ms/step - accuracy: 0.1783 - loss: 1.4187 - val_accuracy: 0.2314 - val_loss: 1.5627\n",
      "Epoch 9/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 161ms/step - accuracy: 0.1770 - loss: 1.3688 - val_accuracy: 0.2314 - val_loss: 1.5659\n",
      "Epoch 10/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 150ms/step - accuracy: 0.1438 - loss: 1.3789 - val_accuracy: 0.0515 - val_loss: 1.5547\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step\n",
      "Robust model classification report on real data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         uc1       0.00      0.00      0.00      3115\n",
      "         uc2       0.14      1.00      0.24       883\n",
      "         uc3       0.00      0.00      0.00      1291\n",
      "         uc4       0.00      0.00      0.00      1089\n",
      "         uc5       0.00      0.00      0.00        81\n",
      "         uc6       1.00      0.99      1.00       266\n",
      "\n",
      "    accuracy                           0.17      6725\n",
      "   macro avg       0.19      0.33      0.21      6725\n",
      "weighted avg       0.06      0.17      0.07      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_robust.compile(\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model_robust.fit(\n",
    "    X_ft,\n",
    "    y_ft_cat,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    class_weight=real_data_class_weight_dict,\n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "y_real_pred_robust = model_robust.predict(X_real_seq)\n",
    "y_real_pred_robust = y_real_pred_robust.argmax(axis=1)\n",
    "print(\"Robust model classification report on real data:\")\n",
    "print(classification_report(y_real_seq, y_real_pred_robust, target_names=list(UC_MAP.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30a9d2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Výsledky modelu s Attention:</b> Model pozornosti bol vyhodnotený na reálnych dátach. Dosiahol presnosť 0.17 na testovacej množine, makro priemerná presnosť je 0.19 a vážená priemerná presnosť je 0.07. Výsledky modelu naznačujú, že nemusí efektívne zachytávať základné vzory v dátach.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
